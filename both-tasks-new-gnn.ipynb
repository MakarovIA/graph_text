{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7ledNLeV0Xc"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMk6f24GV0Xi"
   },
   "outputs": [],
   "source": [
    "\"\"\"GCN using DGL nn package\n",
    "\n",
    "References:\n",
    "- Semi-Supervised Classification with Graph Convolutional Networks\n",
    "- Paper: https://arxiv.org/abs/1609.02907\n",
    "- Code: https://github.com/tkipf/gcn\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "import dgl.function as fn\n",
    "from dgl.nn import GATConv\n",
    "from dgl.nn.pytorch.conv import SAGEConv\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 activation,\n",
    "                 dropout=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.g = g\n",
    "\n",
    "        self.gcn_layer1 = GraphConv(in_feats, n_hidden, activation=activation)\n",
    "\n",
    "        self.gcn_layer2 = GraphConv(n_hidden, n_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, features):\n",
    "        h = features\n",
    "\n",
    "        h = self.gcn_layer1(self.g, h)\n",
    "\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        h = self.gcn_layer2(self.g, h)\n",
    "\n",
    "        return h\n",
    "    \n",
    "    def freeze_features(self, freeze):\n",
    "        self.emb.weight.requires_grad = not freeze\n",
    "\n",
    "    def freeze_graph(self, freeze):\n",
    "        self.gcn_layer1.weight.requires_grad = not freeze\n",
    "        self.gcn_layer2.weight.requires_grad = not freeze\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 activation,\n",
    "                 dropout,\n",
    "                 n_layers,\n",
    "                 heads,\n",
    "                 attn_drop,\n",
    "                 negative_slope,\n",
    "                 residual):\n",
    "        super(GAT, self).__init__()\n",
    "        self.g = g\n",
    "        self.num_layers = n_layers\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        self.activation = activation\n",
    "        # input projection (no residual)\n",
    "        self.gat_layers.append(GATConv(\n",
    "            in_feats, n_hidden, heads[0],\n",
    "            dropout, attn_drop, negative_slope, False, self.activation))\n",
    "        # hidden layers\n",
    "        for l in range(1, n_layers):\n",
    "            # due to multi-head, the in_dim = num_hidden * num_heads\n",
    "            self.gat_layers.append(GATConv(\n",
    "                n_hidden * heads[l - 1], n_hidden, heads[l],\n",
    "                dropout, attn_drop, negative_slope, residual, self.activation))\n",
    "        # output projection\n",
    "        self.gat_layers.append(GATConv(\n",
    "            n_hidden * heads[-2], n_classes, heads[-1],\n",
    "            dropout, attn_drop, negative_slope, residual, None))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h = self.encode(self.g, inputs)\n",
    "        # output projection\n",
    "        logits = self.gat_layers[-1](self.g, h).mean(1)\n",
    "        return logits\n",
    "    \n",
    "    def encode(self, g, inputs):\n",
    "        h = inputs\n",
    "        for l in range(self.num_layers):\n",
    "            h = self.gat_layers[l](g, h).flatten(1)\n",
    "        return h\n",
    "\n",
    "\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 activation,\n",
    "                 dropout,\n",
    "                 n_layers,\n",
    "                 aggregator_type):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.g = g\n",
    "\n",
    "        # input layer\n",
    "        self.layers.append(SAGEConv(in_feats, n_hidden, aggregator_type, feat_drop=dropout, activation=activation))\n",
    "        # hidden layers\n",
    "        for i in range(n_layers - 1):\n",
    "            self.layers.append(SAGEConv(n_hidden, n_hidden, aggregator_type, feat_drop=dropout, activation=activation))\n",
    "        # output layer\n",
    "        self.layers.append(SAGEConv(n_hidden, n_classes, aggregator_type, feat_drop=dropout, activation=None)) # activation None\n",
    "\n",
    "    def forward(self, features):\n",
    "        h = self.encode(self.g, features)\n",
    "        return self.layers[-1](self.g, h)\n",
    "    \n",
    "    def encode(self, g, features):\n",
    "        h = features\n",
    "        for layer in self.layers[:-1]:\n",
    "            h = layer(g, h)\n",
    "        return h\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "Graph InfoClust in DGL\n",
    "Implementation is based on https://github.com/dmlc/dgl/tree/master/examples/pytorch/dgi\n",
    "\"\"\"\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, g, in_feats, n_hidden, n_layers, activation, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.g = g\n",
    "        self.conv = GCN(g, in_feats, n_hidden, n_hidden, activation, dropout)\n",
    "\n",
    "    def forward(self, features, corrupt=False):\n",
    "        if corrupt:\n",
    "            perm = torch.randperm(self.g.number_of_nodes())\n",
    "            features = features[perm]\n",
    "        features = self.conv(features)\n",
    "        return features\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(n_hidden, n_hidden))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def uniform(self, size, tensor):\n",
    "        bound = 1.0 / math.sqrt(size)\n",
    "        if tensor is not None:\n",
    "            tensor.data.uniform_(-bound, bound)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        size = self.weight.size(0)\n",
    "        self.uniform(size, self.weight)\n",
    "\n",
    "    def forward(self, features, summary):\n",
    "        #features @ self.weight @ summary.t()\n",
    "        return torch.matmul(features, torch.matmul(self.weight, summary))\n",
    "    \n",
    "class DiscriminatorK(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(DiscriminatorK, self).__init__()\n",
    "\n",
    "    def forward(self, features, summary):\n",
    "        \n",
    "        n, h = features.size()\n",
    "        \n",
    "        ####features =  features / features.norm(dim=1)[:, None]\n",
    "        #features = torch.sum(features*summary, dim=1)\n",
    "        \n",
    "        #features = features @ self.weight @ summary.t()\n",
    "        return torch.bmm(features.view(n, 1, h), summary.view(n, h, 1)) #torch.sum(features*summary, dim=1) \n",
    "\n",
    "\n",
    "    \n",
    "class GIC(nn.Module):\n",
    "    def __init__(self, g, in_feats, n_hidden, n_classes, activation, dropout, n_layers, K, beta, alpha):\n",
    "        super(GIC, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.g=g\n",
    "        self.encoder = Encoder(g, in_feats, n_hidden, n_layers, activation, dropout)\n",
    "        self.discriminator = Discriminator(n_hidden)\n",
    "        self.discriminator2 = Discriminator(n_hidden)\n",
    "        self.discriminatorK = DiscriminatorK(n_hidden)\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "        self.K = K\n",
    "        self.beta = beta\n",
    "        self.cluster = Clusterator(n_hidden,K)\n",
    "        self.alpha = alpha\n",
    "        self.fc = nn.Linear(n_hidden, n_classes)\n",
    "        \n",
    "\n",
    "    def calc_loss(self, features, mask):\n",
    "        positive = self.encoder(features, corrupt=False)[mask]\n",
    "        negative = self.encoder(features, corrupt=True)[mask]\n",
    "        graph_summary = torch.sigmoid(positive.mean(dim=0))\n",
    "        \n",
    "        mu, r = self.cluster(positive, self.beta)\n",
    "        \n",
    "        \n",
    "        cluster_summary = torch.sigmoid(r @ mu)\n",
    "        \n",
    "        pos_graph = self.discriminator(positive, graph_summary)\n",
    "        neg_graph = self.discriminator(negative, graph_summary)\n",
    "        \n",
    "\n",
    "        l1 = self.loss(pos_graph, torch.ones_like(pos_graph)) \n",
    "        l2 = self.loss(neg_graph, torch.zeros_like(neg_graph)) \n",
    "\n",
    "        l = self.alpha*(l1+l2)\n",
    "        \n",
    "        \n",
    "        pos_cluster = self.discriminatorK(positive, cluster_summary)\n",
    "        neg_cluster = self.discriminatorK(negative, cluster_summary)\n",
    "        \n",
    "        \n",
    "        l += (1-self.alpha)*(self.loss(pos_cluster, torch.ones_like(pos_cluster)) + self.loss(neg_cluster, torch.zeros_like(neg_cluster))) \n",
    "        return l\n",
    "    \n",
    "    def forward(self, features):\n",
    "        return self.fc(self.encoder(features, False))\n",
    "\n",
    "    def encode(self, g, inputs):\n",
    "        return self.encoder(inputs, False)\n",
    "    \n",
    "\n",
    "def cluster(data, k, temp, num_iter, init, cluster_temp):\n",
    "    '''\n",
    "    pytorch (differentiable) implementation of soft k-means clustering. \n",
    "    Modified from https://github.com/bwilder0/clusternet\n",
    "    '''\n",
    "    cuda0 = torch.cuda.is_available()#False\n",
    "    \n",
    "    \n",
    "    \n",
    "    if cuda0:\n",
    "        mu = init.cuda()\n",
    "        data = data.cuda()\n",
    "        cluster_temp = cluster_temp.cuda()\n",
    "    else:\n",
    "        mu = init\n",
    "    n = data.shape[0]\n",
    "    d = data.shape[1]\n",
    "\n",
    "    data = data / (data.norm(dim=1) + 1e-8)[:, None]\n",
    "    \n",
    "    for t in range(num_iter):\n",
    "        #get distances between all data points and cluster centers\n",
    "        \n",
    "        mu = mu / mu.norm(dim=1)[:, None]\n",
    "        dist = torch.mm(data, mu.transpose(0,1))\n",
    "        \n",
    "        \n",
    "        #cluster responsibilities via softmax\n",
    "        r = F.softmax(cluster_temp*dist, dim=1)\n",
    "        #total responsibility of each cluster\n",
    "        cluster_r = r.sum(dim=0)\n",
    "        #mean of points in each cluster weighted by responsibility\n",
    "        cluster_mean = r.t() @ data\n",
    "        #update cluster means\n",
    "        new_mu = torch.diag(1/cluster_r) @ cluster_mean\n",
    "        mu = new_mu\n",
    "        \n",
    "    \n",
    "    \n",
    "    r = F.softmax(cluster_temp*dist, dim=1)\n",
    "    \n",
    "    \n",
    "    return mu, r\n",
    "\n",
    "class Clusterator(nn.Module):\n",
    "    '''\n",
    "    The ClusterNet architecture. The first step is a 2-layer GCN to generate embeddings.\n",
    "    The output is the cluster means mu and soft assignments r, along with the \n",
    "    embeddings and the the node similarities (just output for debugging purposes).\n",
    "    \n",
    "    The forward pass inputs are x, a feature matrix for the nodes, and adj, a sparse\n",
    "    adjacency matrix. The optional parameter num_iter determines how many steps to \n",
    "    run the k-means updates for.\n",
    "    Modified from https://github.com/bwilder0/clusternet\n",
    "    '''\n",
    "    def __init__(self, nout, K):\n",
    "        super(Clusterator, self).__init__()\n",
    "\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.K = K\n",
    "        self.nout = nout\n",
    "        \n",
    "        self.init =  torch.rand(self.K, nout)\n",
    "        \n",
    "    def forward(self, embeds, cluster_temp, num_iter=10):\n",
    "        \n",
    "        mu_init, _ = cluster(embeds, self.K, 1, num_iter, cluster_temp = torch.tensor(cluster_temp), init = self.init)\n",
    "        #self.init = mu_init.clone().detach()\n",
    "        mu, r = cluster(embeds, self.K, 1, 1, cluster_temp = torch.tensor(cluster_temp), init = mu_init.clone().detach())\n",
    "        \n",
    "        return mu, r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4X5lMH9EV0Xj"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWK2bEy3V0Xj"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3h2KGjjjV0Xk"
   },
   "outputs": [],
   "source": [
    "def get_masks(n,\n",
    "              main_ids,\n",
    "              main_labels,\n",
    "              test_ratio,\n",
    "              val_ratio,\n",
    "              seed=1):\n",
    "    \"\"\"\n",
    "    Randomly splits data into train/val/test using random seed\n",
    "    returns masks instead of the data itself  \n",
    "    \"\"\"\n",
    "    train_mask = np.zeros(n)\n",
    "    val_mask = np.zeros(n)\n",
    "    test_mask = np.zeros(n)\n",
    "\n",
    "    x_dev, x_test, y_dev, y_test = train_test_split(main_ids,\n",
    "                                                    main_labels,\n",
    "                                                    stratify=main_labels,\n",
    "                                                    test_size=test_ratio,\n",
    "                                                    random_state=seed)\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_dev,\n",
    "                                                      y_dev,\n",
    "                                                      stratify=y_dev,\n",
    "                                                      test_size=val_ratio,\n",
    "                                                      random_state=seed)\n",
    "\n",
    "    train_mask[x_train] = 1\n",
    "    val_mask[x_val] = 1\n",
    "    test_mask[x_test] = 1\n",
    "\n",
    "    return train_mask, val_mask, test_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uT_207WqV0Xk"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, features, labels, mask):\n",
    "    \"\"\"\n",
    "    Evaluate model quality (F1-score)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask].detach().cpu().numpy()\n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        predicted = predicted.detach().cpu().numpy()\n",
    "        f1 = f1_score(labels, predicted, average='micro')\n",
    "        return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nwul_O3_V0Xl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dgl\n",
    "from dgl import DGLGraph\n",
    "\n",
    "def train_gcn_lp(graph,\n",
    "              features,\n",
    "              labels,\n",
    "              model,\n",
    "              args,\n",
    "              seed=1,\n",
    "              n_hidden=64,\n",
    "              n_epochs=200,\n",
    "              lr=1e-2,\n",
    "              weight_decay=5e-4,\n",
    "              dropout=0.5,\n",
    "              verbose=False,\n",
    "              cuda=False, modelType=\"GAT\"):\n",
    "\n",
    "    features = torch.FloatTensor(features)\n",
    "    labels = torch.LongTensor(labels)\n",
    "    \n",
    "    mask = []\n",
    "    for i in range(len(labels)):\n",
    "        # nodes with labels\n",
    "        if graph.nodes[i]['is_main']:\n",
    "            mask.append(1)\n",
    "        else:\n",
    "            mask.append(0)\n",
    "            \n",
    "    mask = torch.BoolTensor(mask)\n",
    "        \n",
    "\n",
    "    if cuda:\n",
    "        torch.cuda.set_device(\"cuda:0\")\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        train_mask = train_mask.cuda()\n",
    "        val_mask = val_mask.cuda()\n",
    "        test_mask = test_mask.cuda()\n",
    "\n",
    "    g = DGLGraph(graph)\n",
    "    g = dgl.transform.add_self_loop(g)\n",
    "    n_edges = g.number_of_edges()\n",
    "\n",
    "    degs = g.in_degrees().float()\n",
    "    norm = torch.pow(degs, -0.5)\n",
    "    norm[torch.isinf(norm)] = 0\n",
    "\n",
    "    if cuda:\n",
    "        norm = norm.cuda()\n",
    "\n",
    "    g.ndata['norm'] = norm.unsqueeze(1)\n",
    "\n",
    "    in_feats = features.shape[1]\n",
    "\n",
    "    # + 1 for unknown class\n",
    "    n_classes = len(np.unique(labels))\n",
    "    \n",
    "    ##########\n",
    "    ##########  HERE WE USE MODEL\n",
    "    ##########\n",
    "    model = model(g,\n",
    "                in_feats=in_feats,\n",
    "                n_hidden=n_hidden,\n",
    "                n_classes=n_classes,\n",
    "                activation=F.relu,\n",
    "                dropout=dropout, **args)\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # use optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                           mode='min',\n",
    "                                                           factor=0.9,\n",
    "                                                           patience=20,\n",
    "                                                           min_lr=1e-10)\n",
    "\n",
    "    best_f1 = -100\n",
    "    # initialize graph\n",
    "    dur = []\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        if epoch >= 3:\n",
    "            t0 = time.time()\n",
    "        logits = model(features)\n",
    "        loss = loss_fcn(logits[mask], labels[mask])\n",
    "        if modelType == \"GIC\":\n",
    "            loss += model.calc_loss(features, mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch >= 3:\n",
    "            dur.append(time.time() - t0)\n",
    "\n",
    "        f1 = evaluate(model, features, labels, mask)\n",
    "        scheduler.step(1 - f1)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | F1 {:.4f} | \"\n",
    "                  \"ETputs(KTEPS) {:.2f}\".format(epoch, np.mean(dur), loss.item(),\n",
    "                                                f1, n_edges / np.mean(dur) / 1000))\n",
    "\n",
    "    model.load_state_dict(torch.load('best_model.pt'))\n",
    "    \n",
    "    embeddings = model.encode(model.g, features).detach().cpu().numpy()\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1-5KB9tV0Xl"
   },
   "source": [
    "## LP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGEJdDHtV0Xm"
   },
   "outputs": [],
   "source": [
    "class GCN_Model_LP:\n",
    "    def __init__(self, graph, features, model, args, labels=None, dim=80, modelName=\"GAT\"):\n",
    "        self.graph = graph\n",
    "        self.features = features\n",
    "        self.dim = dim\n",
    "        self.labels = labels\n",
    "        self.embeddings = None\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "        self.modelName = modelName\n",
    "\n",
    "    def learn_embeddings(self):\n",
    "        embeddings = train_gcn_lp(self.graph, self.features, self.labels, self.model, self.args, self.modelName)\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def get_embeddings_for_ids(self, ids):\n",
    "        result = []\n",
    "        for i, embedding in enumerate(self.embeddings):\n",
    "            if i in ids:\n",
    "                result.append(embedding)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfxMSKTQV0Xm"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_DsS_NjV0Xm"
   },
   "outputs": [],
   "source": [
    "from datasets import Cora, CiteseerM10, Dblp\n",
    "\n",
    "datasets = [\n",
    "   ('Cora', Cora),\n",
    "   ('CiteseerM10', CiteseerM10),\n",
    "   ('DBLP', Dblp)\n",
    "]\n",
    "\n",
    "\n",
    "from text_transformers import SBert, LDA, W2V, Sent2Vec, Doc2Vec, BOW, TFIDF\n",
    "\n",
    "gatargs = {\n",
    "              \"n_layers\": 1,\n",
    "              \"heads\": [8, 1],\n",
    "              \"attn_drop\": 0.6,\n",
    "              \"negative_slope\": 0.2,  \n",
    "              \"residual\": False\n",
    "          }\n",
    "\n",
    "gsargs = {\"n_layers\": 2, \"aggregator_type\": \"mean\"}\n",
    "gicargs = {\n",
    "                \"n_layers\": 1,\n",
    "                \"K\": 128,\n",
    "                \"beta\": 100,\n",
    "                \"alpha\": 0.5\n",
    "            }\n",
    "\n",
    "tasks = [\n",
    "    ('GAT(W2V)', lambda ds: LpTask(ds, test_ratios, W2V, GCN_Model_LP, GAT, gatargs, d=100, labels=True)),\n",
    "    ('GraphSAGE(W2V)', lambda ds: LpTask(ds, test_ratios, W2V, GCN_Model_LP, GraphSAGE, gsargs, d=100, labels=True)),\n",
    "    ('GIC(W2V)', lambda ds: LpTask(ds, test_ratios, TFIDF, GCN_Model_LP, GIC, gicargs, d=100, labels=True, modelName=\"GIC\")),\n",
    "    ('GAT(S-BERT)', lambda ds: LpTask(ds, test_ratios, SBert, GCN_Model_LP, GAT, gatargs, d=100, labels=True)),\n",
    "    ('GraphSAGE(S-BERT)', lambda ds: LpTask(ds, test_ratios, SBert, GCN_Model_LP, GraphSAGE, gsargs, d=100, labels=True)),\n",
    "    ('GIC(S-BERT)', lambda ds: LpTask(ds, test_ratios, TFIDF, GCN_Model_LP, GIC, gicargs, d=100, labels=True, modelName=\"GIC\")),\n",
    "    ('GAT(TFIDF)', lambda ds: LpTask(ds, test_ratios, TFIDF, GCN_Model_LP, GAT, gatargs, d=100, labels=True)),\n",
    "    ('GraphSAGE(TFIDF)', lambda ds: LpTask(ds, test_ratios, TFIDF, GCN_Model_LP, GraphSAGE, gsargs, d=100, labels=True)),\n",
    "    ('GIC(TFIDF)', lambda ds: LpTask(ds, test_ratios, TFIDF, GCN_Model_LP, GIC, gicargs, d=100, labels=True, modelName=\"GIC\")),\n",
    "    ('GAT(Sent2Vec)', lambda ds: LpTask(ds, test_ratios, Sent2Vec, GCN_Model_LP, GAT, gicargs, d=100, labels=True)),\n",
    "    ('GraphSAGE(Sent2Vec)', lambda ds: LpTask(ds, test_ratios, Sent2Vec, GCN_Model_LP, GraphSAGE, gicargs, d=100, labels=True)),\n",
    "    ('GIC(Sent2Vec)',lambda ds: LpTask(ds, test_ratios, Sent2Vec, GCN_Model_LP, GIC, gicargs, d=100, labels=True, modelName=\"GIC\")),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQ6vV3iiV0Xn"
   },
   "outputs": [],
   "source": [
    "test_ratios = [0.5, 0.7, 0.9, 0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OHreCDOSV0Xn",
    "outputId": "81d83417-3c71-47a6-ea91-d2b7c5eef5a5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from task import LpTask\n",
    "\n",
    "res = {}\n",
    "\n",
    "for ds_name, ds_constr in tqdm(datasets, desc='datasets'):\n",
    "    ds = ds_constr()\n",
    "    for task_name, task_constr in tqdm(tasks, desc='Tasks'):\n",
    "        task = task_constr(ds)\n",
    "        task_res = task.evaluate()\n",
    "        for test_ratio in task_res:\n",
    "            scores = task_res[test_ratio]\n",
    "            res[f'{1 - test_ratio:.2f} - {ds_name} - {task_name}'] = scores\n",
    "\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def beautify_results(res):\n",
    "    res2 = []\n",
    "    for name, scores in res.items():\n",
    "        res2.append({\"name\": name, \"mn\": np.mean(scores), \"error\": np.std(scores)})\n",
    "    df = pd.DataFrame(res2)\n",
    "    df[\"test_ratio\"] = df.name.str.split(\" - \").str[0]\n",
    "    df[\"dataset\"] = df.name.str.split(\" - \").str[1]\n",
    "    df[\"graph_model\"] = df.name.str.split(\" - \").str[2].str.split().str[0]\n",
    "    df[\"text_model\"] = df.name.str.split(\" - \").str[2].str.split().str[1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = beautify_results(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dgl\n",
    "from dgl import DGLGraph\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def get_masks(n,\n",
    "              main_ids,\n",
    "              main_labels,\n",
    "              test_ratio,\n",
    "              val_ratio,\n",
    "              seed=1):\n",
    "    \"\"\"\n",
    "    Randomly splits data into train/val/test using random seed\n",
    "    returns masks instead of the data itself  \n",
    "    \"\"\"\n",
    "    train_mask = np.zeros(n)\n",
    "    val_mask = np.zeros(n)\n",
    "    test_mask = np.zeros(n)\n",
    "\n",
    "    x_dev, x_test, y_dev, y_test = train_test_split(main_ids,\n",
    "                                                    main_labels,\n",
    "                                                    stratify=main_labels,\n",
    "                                                    test_size=test_ratio,\n",
    "                                                    random_state=seed)\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_dev,\n",
    "                                                      y_dev,\n",
    "                                                      stratify=y_dev,\n",
    "                                                      test_size=val_ratio,\n",
    "                                                      random_state=seed)\n",
    "\n",
    "    train_mask[x_train] = 1\n",
    "    val_mask[x_val] = 1\n",
    "    test_mask[x_test] = 1\n",
    "\n",
    "    return train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "def evaluate(model, features, labels, mask):\n",
    "    \"\"\"\n",
    "    Evaluate model quality (F1-score)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask].detach().cpu().numpy()\n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        predicted = predicted.detach().cpu().numpy()\n",
    "        f1 = f1_score(labels, predicted, average='micro')\n",
    "        return f1\n",
    "\n",
    "def train_gcn(dataset,\n",
    "              test_ratio=0.5,\n",
    "              val_ratio=0.2,\n",
    "              seed=1,\n",
    "              n_hidden=64,\n",
    "              n_epochs=200,\n",
    "              lr=1e-2,\n",
    "              weight_decay=5e-4,\n",
    "              dropout=0.5,\n",
    "              verbose=True,\n",
    "              cuda=False, MODEL=GCN, gat_args={}, modelType = \"GAT\"):\n",
    "    data = dataset.get_data()\n",
    "\n",
    "    features = torch.FloatTensor(data['features'])\n",
    "    labels = torch.LongTensor(data['labels'])\n",
    "\n",
    "    n = len(data['ids'])\n",
    "    train_mask, val_mask, test_mask = get_masks(n,\n",
    "                                                data['main_ids'],\n",
    "                                                data['main_labels'],\n",
    "                                                test_ratio=test_ratio,\n",
    "                                                val_ratio=val_ratio,\n",
    "                                                seed=seed)\n",
    "\n",
    "    train_mask = torch.BoolTensor(train_mask)\n",
    "    val_mask = torch.BoolTensor(val_mask)\n",
    "    test_mask = torch.BoolTensor(test_mask)\n",
    "\n",
    "    if cuda:\n",
    "        torch.cuda.set_device(\"cuda:0\")\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        train_mask = train_mask.cuda()\n",
    "        val_mask = val_mask.cuda()\n",
    "        test_mask = test_mask.cuda()\n",
    "\n",
    "    g = DGLGraph(data['graph'])\n",
    "    g = dgl.transform.add_self_loop(g)\n",
    "    n_edges = g.number_of_edges()\n",
    "\n",
    "    degs = g.in_degrees().float()\n",
    "    norm = torch.pow(degs, -0.5)\n",
    "    norm[torch.isinf(norm)] = 0\n",
    "\n",
    "    if cuda:\n",
    "        norm = norm.cuda()\n",
    "\n",
    "    g.ndata['norm'] = norm.unsqueeze(1)\n",
    "\n",
    "    in_feats = features.shape[1]\n",
    "\n",
    "    # + 1 for unknown class\n",
    "    n_classes = data['n_classes'] + 1\n",
    "    \n",
    "    ##########\n",
    "    ##########  HERE WE USE MODEL\n",
    "    ##########\n",
    "    model = MODEL(g,\n",
    "                in_feats=in_feats,\n",
    "                n_hidden=n_hidden,\n",
    "                n_classes=n_classes,\n",
    "                activation=F.relu,\n",
    "                dropout=dropout, **gat_args)\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # use optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                           mode='min',\n",
    "                                                           factor=0.9,\n",
    "                                                           patience=20,\n",
    "                                                           min_lr=1e-10)\n",
    "\n",
    "    best_f1 = -100\n",
    "    # initialize graph\n",
    "    dur = []\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        if epoch >= 3:\n",
    "            t0 = time.time()\n",
    "        # forward\n",
    "        logits = model(features)\n",
    "        loss = loss_fcn(logits[train_mask], labels[train_mask])\n",
    "        if modelType == \"GIC\":\n",
    "            loss += model.calc_loss(features, train_mask)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch >= 3:\n",
    "            dur.append(time.time() - t0)\n",
    "\n",
    "        f1 = evaluate(model, features, labels, val_mask)\n",
    "        scheduler.step(1 - f1)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | F1 {:.4f} | \"\n",
    "                  \"ETputs(KTEPS) {:.2f}\".format(epoch, np.mean(dur), loss.item(),\n",
    "                                                f1, n_edges / np.mean(dur) / 1000))\n",
    "\n",
    "    model.load_state_dict(torch.load('best_model.pt'))\n",
    "    f1 = evaluate(model, features, labels, test_mask)\n",
    "\n",
    "    if verbose:\n",
    "        print()\n",
    "        print(\"Test F1 {:.2}\".format(f1))\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Cora, CiteseerM10, Dblp\n",
    "\n",
    "datasets = [\n",
    "   ('Cora', Cora),\n",
    "   ('CiteseerM10', CiteseerM10),\n",
    "   ('DBLP', Dblp)\n",
    "]\n",
    "\n",
    "\n",
    "from text_transformers import SBert, LDA, W2V, Sent2Vec, Doc2Vec, BOW, TFIDF\n",
    "\n",
    "text_transformers = [\n",
    "    (\"TFIDF\", TFIDF()),\n",
    "    (\"W2V(d=300)\", W2V(train=False, d=300)),\n",
    "    (\"S-BERT(d=300)\", SBert(train=False, d=300)),\n",
    "    (\"Sent2Vec(d=600)\", Sent2Vec(train=False, d=600)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1, 2]\n",
    "test_ratios = [0.5, 0.7, 0.9, 0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (GAT, \"GAT\", {\n",
    "      \"n_layers\": 1,\n",
    "      \"heads\": [8, 1],\n",
    "      \"attn_drop\": 0.6,\n",
    "      \"negative_slope\": 0.2,  \n",
    "      \"residual\": False\n",
    "    }),\n",
    "    (GIC, \"GIC\", {\n",
    "        \"n_layers\": 1,\n",
    "        \"K\": 128,\n",
    "        \"beta\": 100,\n",
    "        \"alpha\": 0.5\n",
    "    }),\n",
    "    (GraphSAGE, \"GraphSAGE\", {\"n_layers\": 2, \"aggregator_type\": \"mean\"})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "res = {}\n",
    "for ds_name, ds_constr in tqdm(datasets, desc='datasets'):\n",
    "    ds = ds_constr()\n",
    "    for text_trans_name, text_transofmer in tqdm(text_transformers, \"transformers\"):\n",
    "        ds.transform_features(text_transofmer)\n",
    "        for model, m_name, args in models:\n",
    "            for test_ratio in tqdm(test_ratios, desc='test ratio'):\n",
    "                scores = []\n",
    "                for seed in seeds:\n",
    "                    score = train_gcn(ds, test_ratio, seed=seed, verbose=False, MODEL=model, gat_args=args, modelType=m_name)\n",
    "                    scores.append(score)\n",
    "\n",
    "                nm = f'{1 - test_ratio:.2f} - {ds_name} - {m_name} {text_trans_name}'\n",
    "                res[nm] = scores\n",
    "                print()\n",
    "                print(nm, scores, np.mean(scores), np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = beautify_results(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Копия блокнота \"link_prediction_template.ipynb\"",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
