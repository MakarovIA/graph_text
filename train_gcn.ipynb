{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from dgl import DGLGraph\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from models import GCN\n",
    "from datasets import Cora, CiteseerM10, Dblp\n",
    "from text_transformers import TFIDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(n, main_ids, main_labels, test_ratio, val_ratio, seed=1):\n",
    "    train_mask = np.zeros(n)\n",
    "    val_mask = np.zeros(n)\n",
    "    test_mask = np.zeros(n)\n",
    "\n",
    "    x_dev, x_test, y_dev, y_test = train_test_split(main_ids,\n",
    "                                                    main_labels,\n",
    "                                                    stratify=main_labels,\n",
    "                                                    test_size=test_ratio,\n",
    "                                                    random_state=seed)\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_dev,\n",
    "                                                      y_dev,\n",
    "                                                      stratify=y_dev,\n",
    "                                                      test_size=val_ratio,\n",
    "                                                      random_state=seed)\n",
    "\n",
    "    train_mask[x_train] = 1\n",
    "    val_mask[x_val] = 1\n",
    "    test_mask[x_test] = 1\n",
    "\n",
    "    return train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "def evaluate(model, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        f1 = f1_score(labels, predicted, average='micro')\n",
    "        return f1\n",
    "\n",
    "\n",
    "def train_gcn(dataset,\n",
    "              test_ratio=0.5,\n",
    "              val_ratio=0.2,\n",
    "              seed=1,\n",
    "              n_hidden=16,\n",
    "              n_epochs=200,\n",
    "              lr=1e-2,\n",
    "              weight_decay=5e-4,\n",
    "              dropout=0.5,\n",
    "              verbose=True):\n",
    "    data = dataset.get_data()\n",
    "    features = torch.FloatTensor(data['features'])\n",
    "    labels = torch.LongTensor(data['labels'])\n",
    "    n = len(data['ids'])\n",
    "    train_mask, val_mask, test_mask = get_masks(n,\n",
    "                                                data['main_ids'],\n",
    "                                                data['main_labels'],\n",
    "                                                test_ratio=test_ratio,\n",
    "                                                val_ratio=val_ratio,\n",
    "                                                seed=seed)\n",
    "\n",
    "    train_mask = torch.BoolTensor(train_mask)\n",
    "    val_mask = torch.BoolTensor(val_mask)\n",
    "    test_mask = torch.BoolTensor(test_mask)\n",
    "\n",
    "    g = DGLGraph(data['graph'])\n",
    "    n_edges = g.number_of_edges()\n",
    "\n",
    "    degs = g.in_degrees().float()\n",
    "    norm = torch.pow(degs, -0.5)\n",
    "    norm[torch.isinf(norm)] = 0\n",
    "\n",
    "    g.ndata['norm'] = norm.unsqueeze(1)\n",
    "\n",
    "    in_feats = features.shape[1]\n",
    "    # + 1 for unknown class\n",
    "    n_classes = data['n_classes'] + 1\n",
    "    model = GCN(g,\n",
    "                in_feats=in_feats,\n",
    "                n_hidden=n_hidden,\n",
    "                n_classes=n_classes,\n",
    "                activation=F.relu,\n",
    "                dropout=dropout)\n",
    "\n",
    "    loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # use optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "\n",
    "    # initialize graph\n",
    "    dur = []\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        if epoch >= 3:\n",
    "            t0 = time.time()\n",
    "        # forward\n",
    "        logits = model(features)\n",
    "        loss = loss_fcn(logits[train_mask], labels[train_mask])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch >= 3:\n",
    "            dur.append(time.time() - t0)\n",
    "\n",
    "        f1 = evaluate(model, features, labels, val_mask)\n",
    "        if verbose:\n",
    "            print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | F1 {:.4f} | \"\n",
    "                  \"ETputs(KTEPS) {:.2f}\".format(epoch, np.mean(dur), loss.item(),\n",
    "                                                f1, n_edges / np.mean(dur) / 1000))\n",
    "\n",
    "    f1 = evaluate(model, features, labels, test_mask)\n",
    "\n",
    "    if verbose:\n",
    "        print()\n",
    "        print(\"Test F1 {:.2}\".format(f1))\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikhail-makarov/opt/anaconda3/envs/graph_text/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/mikhail-makarov/opt/anaconda3/envs/graph_text/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Time(s) nan | Loss 2.0774 | F1 0.3727 | ETputs(KTEPS) nan\n",
      "Epoch 00001 | Time(s) nan | Loss 2.0267 | F1 0.3506 | ETputs(KTEPS) nan\n",
      "Epoch 00002 | Time(s) nan | Loss 1.9662 | F1 0.3469 | ETputs(KTEPS) nan\n",
      "Epoch 00003 | Time(s) 0.0401 | Loss 1.9052 | F1 0.3395 | ETputs(KTEPS) 262.93\n",
      "Epoch 00004 | Time(s) 0.0369 | Loss 1.8542 | F1 0.3395 | ETputs(KTEPS) 285.84\n",
      "Epoch 00005 | Time(s) 0.0364 | Loss 1.7922 | F1 0.3395 | ETputs(KTEPS) 289.92\n",
      "Epoch 00006 | Time(s) 0.0348 | Loss 1.7406 | F1 0.3469 | ETputs(KTEPS) 303.29\n",
      "Epoch 00007 | Time(s) 0.0336 | Loss 1.6714 | F1 0.3469 | ETputs(KTEPS) 314.02\n",
      "Epoch 00008 | Time(s) 0.0328 | Loss 1.6306 | F1 0.3506 | ETputs(KTEPS) 321.85\n",
      "Epoch 00009 | Time(s) 0.0329 | Loss 1.5674 | F1 0.3579 | ETputs(KTEPS) 321.00\n",
      "Epoch 00010 | Time(s) 0.0339 | Loss 1.5402 | F1 0.3616 | ETputs(KTEPS) 311.32\n",
      "Epoch 00011 | Time(s) 0.0340 | Loss 1.5083 | F1 0.4022 | ETputs(KTEPS) 310.64\n",
      "Epoch 00012 | Time(s) 0.0344 | Loss 1.4579 | F1 0.4539 | ETputs(KTEPS) 306.60\n",
      "Epoch 00013 | Time(s) 0.0339 | Loss 1.3954 | F1 0.5055 | ETputs(KTEPS) 311.47\n",
      "Epoch 00014 | Time(s) 0.0334 | Loss 1.3628 | F1 0.5351 | ETputs(KTEPS) 316.38\n",
      "Epoch 00015 | Time(s) 0.0330 | Loss 1.3126 | F1 0.5720 | ETputs(KTEPS) 319.61\n",
      "Epoch 00016 | Time(s) 0.0325 | Loss 1.2714 | F1 0.6162 | ETputs(KTEPS) 325.06\n",
      "Epoch 00017 | Time(s) 0.0321 | Loss 1.2104 | F1 0.6458 | ETputs(KTEPS) 328.33\n",
      "Epoch 00018 | Time(s) 0.0319 | Loss 1.1750 | F1 0.6827 | ETputs(KTEPS) 330.51\n",
      "Epoch 00019 | Time(s) 0.0323 | Loss 1.1533 | F1 0.7122 | ETputs(KTEPS) 326.57\n",
      "Epoch 00020 | Time(s) 0.0326 | Loss 1.0994 | F1 0.7343 | ETputs(KTEPS) 323.41\n",
      "Epoch 00021 | Time(s) 0.0334 | Loss 1.0712 | F1 0.7454 | ETputs(KTEPS) 316.29\n",
      "Epoch 00022 | Time(s) 0.0332 | Loss 1.0259 | F1 0.7712 | ETputs(KTEPS) 318.13\n",
      "Epoch 00023 | Time(s) 0.0328 | Loss 0.9949 | F1 0.7675 | ETputs(KTEPS) 322.02\n",
      "Epoch 00024 | Time(s) 0.0323 | Loss 0.9718 | F1 0.7749 | ETputs(KTEPS) 326.37\n",
      "Epoch 00025 | Time(s) 0.0320 | Loss 0.9311 | F1 0.7712 | ETputs(KTEPS) 330.16\n",
      "Epoch 00026 | Time(s) 0.0317 | Loss 0.9098 | F1 0.7823 | ETputs(KTEPS) 332.56\n",
      "Epoch 00027 | Time(s) 0.0314 | Loss 0.8560 | F1 0.7860 | ETputs(KTEPS) 335.78\n",
      "Epoch 00028 | Time(s) 0.0315 | Loss 0.8293 | F1 0.7934 | ETputs(KTEPS) 335.17\n",
      "Epoch 00029 | Time(s) 0.0313 | Loss 0.8319 | F1 0.8044 | ETputs(KTEPS) 337.29\n",
      "Epoch 00030 | Time(s) 0.0311 | Loss 0.8252 | F1 0.8081 | ETputs(KTEPS) 339.58\n",
      "Epoch 00031 | Time(s) 0.0309 | Loss 0.7929 | F1 0.8155 | ETputs(KTEPS) 341.63\n",
      "Epoch 00032 | Time(s) 0.0307 | Loss 0.7280 | F1 0.8229 | ETputs(KTEPS) 343.93\n",
      "Epoch 00033 | Time(s) 0.0307 | Loss 0.7444 | F1 0.8229 | ETputs(KTEPS) 343.25\n",
      "Epoch 00034 | Time(s) 0.0306 | Loss 0.7126 | F1 0.8229 | ETputs(KTEPS) 344.79\n",
      "Epoch 00035 | Time(s) 0.0304 | Loss 0.7076 | F1 0.8266 | ETputs(KTEPS) 346.66\n",
      "Epoch 00036 | Time(s) 0.0304 | Loss 0.7013 | F1 0.8303 | ETputs(KTEPS) 347.57\n",
      "Epoch 00037 | Time(s) 0.0301 | Loss 0.6619 | F1 0.8303 | ETputs(KTEPS) 350.06\n",
      "Epoch 00038 | Time(s) 0.0301 | Loss 0.6256 | F1 0.8266 | ETputs(KTEPS) 350.77\n",
      "Epoch 00039 | Time(s) 0.0301 | Loss 0.6255 | F1 0.8266 | ETputs(KTEPS) 350.56\n",
      "Epoch 00040 | Time(s) 0.0300 | Loss 0.6122 | F1 0.8266 | ETputs(KTEPS) 351.44\n",
      "Epoch 00041 | Time(s) 0.0300 | Loss 0.6106 | F1 0.8266 | ETputs(KTEPS) 351.49\n",
      "Epoch 00042 | Time(s) 0.0299 | Loss 0.6079 | F1 0.8266 | ETputs(KTEPS) 353.37\n",
      "Epoch 00043 | Time(s) 0.0298 | Loss 0.5849 | F1 0.8339 | ETputs(KTEPS) 354.44\n",
      "Epoch 00044 | Time(s) 0.0297 | Loss 0.5768 | F1 0.8376 | ETputs(KTEPS) 355.27\n",
      "Epoch 00045 | Time(s) 0.0296 | Loss 0.5850 | F1 0.8376 | ETputs(KTEPS) 356.67\n",
      "Epoch 00046 | Time(s) 0.0296 | Loss 0.5589 | F1 0.8376 | ETputs(KTEPS) 356.50\n",
      "Epoch 00047 | Time(s) 0.0295 | Loss 0.5417 | F1 0.8339 | ETputs(KTEPS) 357.44\n",
      "Epoch 00048 | Time(s) 0.0294 | Loss 0.4949 | F1 0.8339 | ETputs(KTEPS) 359.02\n",
      "Epoch 00049 | Time(s) 0.0293 | Loss 0.5233 | F1 0.8339 | ETputs(KTEPS) 359.73\n",
      "Epoch 00050 | Time(s) 0.0292 | Loss 0.5002 | F1 0.8450 | ETputs(KTEPS) 361.13\n",
      "Epoch 00051 | Time(s) 0.0291 | Loss 0.5069 | F1 0.8450 | ETputs(KTEPS) 362.45\n",
      "Epoch 00052 | Time(s) 0.0290 | Loss 0.5111 | F1 0.8413 | ETputs(KTEPS) 363.65\n",
      "Epoch 00053 | Time(s) 0.0289 | Loss 0.4924 | F1 0.8376 | ETputs(KTEPS) 364.83\n",
      "Epoch 00054 | Time(s) 0.0290 | Loss 0.4977 | F1 0.8376 | ETputs(KTEPS) 364.53\n",
      "Epoch 00055 | Time(s) 0.0289 | Loss 0.4706 | F1 0.8376 | ETputs(KTEPS) 365.12\n",
      "Epoch 00056 | Time(s) 0.0288 | Loss 0.4996 | F1 0.8413 | ETputs(KTEPS) 366.29\n",
      "Epoch 00057 | Time(s) 0.0289 | Loss 0.4563 | F1 0.8450 | ETputs(KTEPS) 365.13\n",
      "Epoch 00058 | Time(s) 0.0289 | Loss 0.4608 | F1 0.8450 | ETputs(KTEPS) 364.83\n",
      "Epoch 00059 | Time(s) 0.0290 | Loss 0.4506 | F1 0.8450 | ETputs(KTEPS) 363.85\n",
      "Epoch 00060 | Time(s) 0.0291 | Loss 0.4483 | F1 0.8450 | ETputs(KTEPS) 363.21\n",
      "Epoch 00061 | Time(s) 0.0291 | Loss 0.4258 | F1 0.8487 | ETputs(KTEPS) 363.00\n",
      "Epoch 00062 | Time(s) 0.0291 | Loss 0.4423 | F1 0.8450 | ETputs(KTEPS) 362.59\n",
      "Epoch 00063 | Time(s) 0.0292 | Loss 0.4471 | F1 0.8450 | ETputs(KTEPS) 361.90\n",
      "Epoch 00064 | Time(s) 0.0292 | Loss 0.4389 | F1 0.8376 | ETputs(KTEPS) 360.86\n",
      "Epoch 00065 | Time(s) 0.0292 | Loss 0.4114 | F1 0.8413 | ETputs(KTEPS) 360.89\n",
      "Epoch 00066 | Time(s) 0.0293 | Loss 0.4164 | F1 0.8450 | ETputs(KTEPS) 360.75\n",
      "Epoch 00067 | Time(s) 0.0293 | Loss 0.4357 | F1 0.8450 | ETputs(KTEPS) 360.25\n",
      "Epoch 00068 | Time(s) 0.0293 | Loss 0.4348 | F1 0.8413 | ETputs(KTEPS) 359.96\n",
      "Epoch 00069 | Time(s) 0.0293 | Loss 0.4102 | F1 0.8450 | ETputs(KTEPS) 360.01\n",
      "Epoch 00070 | Time(s) 0.0293 | Loss 0.4079 | F1 0.8413 | ETputs(KTEPS) 359.94\n",
      "Epoch 00071 | Time(s) 0.0293 | Loss 0.4396 | F1 0.8413 | ETputs(KTEPS) 360.01\n",
      "Epoch 00072 | Time(s) 0.0294 | Loss 0.4118 | F1 0.8487 | ETputs(KTEPS) 359.33\n",
      "Epoch 00073 | Time(s) 0.0294 | Loss 0.3957 | F1 0.8450 | ETputs(KTEPS) 359.35\n",
      "Epoch 00074 | Time(s) 0.0294 | Loss 0.3850 | F1 0.8450 | ETputs(KTEPS) 359.29\n",
      "Epoch 00075 | Time(s) 0.0294 | Loss 0.3970 | F1 0.8376 | ETputs(KTEPS) 358.72\n",
      "Epoch 00076 | Time(s) 0.0295 | Loss 0.4084 | F1 0.8376 | ETputs(KTEPS) 358.30\n",
      "Epoch 00077 | Time(s) 0.0296 | Loss 0.4029 | F1 0.8413 | ETputs(KTEPS) 357.15\n",
      "Epoch 00078 | Time(s) 0.0296 | Loss 0.3864 | F1 0.8413 | ETputs(KTEPS) 356.87\n",
      "Epoch 00079 | Time(s) 0.0296 | Loss 0.3889 | F1 0.8413 | ETputs(KTEPS) 356.84\n",
      "Epoch 00080 | Time(s) 0.0297 | Loss 0.3803 | F1 0.8376 | ETputs(KTEPS) 355.90\n",
      "Epoch 00081 | Time(s) 0.0296 | Loss 0.3981 | F1 0.8376 | ETputs(KTEPS) 356.00\n",
      "Epoch 00082 | Time(s) 0.0296 | Loss 0.3746 | F1 0.8339 | ETputs(KTEPS) 355.98\n",
      "Epoch 00083 | Time(s) 0.0296 | Loss 0.3480 | F1 0.8339 | ETputs(KTEPS) 356.02\n",
      "Epoch 00084 | Time(s) 0.0296 | Loss 0.3806 | F1 0.8339 | ETputs(KTEPS) 356.54\n",
      "Epoch 00085 | Time(s) 0.0297 | Loss 0.3868 | F1 0.8413 | ETputs(KTEPS) 355.85\n",
      "Epoch 00086 | Time(s) 0.0296 | Loss 0.3765 | F1 0.8376 | ETputs(KTEPS) 356.28\n",
      "Epoch 00087 | Time(s) 0.0296 | Loss 0.3675 | F1 0.8376 | ETputs(KTEPS) 356.08\n",
      "Epoch 00088 | Time(s) 0.0296 | Loss 0.3678 | F1 0.8339 | ETputs(KTEPS) 355.96\n",
      "Epoch 00089 | Time(s) 0.0296 | Loss 0.3663 | F1 0.8303 | ETputs(KTEPS) 356.60\n",
      "Epoch 00090 | Time(s) 0.0296 | Loss 0.3789 | F1 0.8376 | ETputs(KTEPS) 356.86\n",
      "Epoch 00091 | Time(s) 0.0296 | Loss 0.3524 | F1 0.8376 | ETputs(KTEPS) 356.43\n",
      "Epoch 00092 | Time(s) 0.0296 | Loss 0.3527 | F1 0.8450 | ETputs(KTEPS) 356.49\n",
      "Epoch 00093 | Time(s) 0.0296 | Loss 0.3580 | F1 0.8450 | ETputs(KTEPS) 356.51\n",
      "Epoch 00094 | Time(s) 0.0296 | Loss 0.3588 | F1 0.8450 | ETputs(KTEPS) 356.52\n",
      "Epoch 00095 | Time(s) 0.0296 | Loss 0.3341 | F1 0.8450 | ETputs(KTEPS) 356.57\n",
      "Epoch 00096 | Time(s) 0.0296 | Loss 0.3526 | F1 0.8450 | ETputs(KTEPS) 356.05\n",
      "Epoch 00097 | Time(s) 0.0296 | Loss 0.3347 | F1 0.8487 | ETputs(KTEPS) 356.14\n",
      "Epoch 00098 | Time(s) 0.0297 | Loss 0.3540 | F1 0.8487 | ETputs(KTEPS) 355.93\n",
      "Epoch 00099 | Time(s) 0.0297 | Loss 0.3470 | F1 0.8450 | ETputs(KTEPS) 355.79\n",
      "Epoch 00100 | Time(s) 0.0297 | Loss 0.3462 | F1 0.8376 | ETputs(KTEPS) 355.85\n",
      "Epoch 00101 | Time(s) 0.0296 | Loss 0.3590 | F1 0.8339 | ETputs(KTEPS) 356.31\n",
      "Epoch 00102 | Time(s) 0.0296 | Loss 0.3427 | F1 0.8339 | ETputs(KTEPS) 356.37\n",
      "Epoch 00103 | Time(s) 0.0296 | Loss 0.3282 | F1 0.8376 | ETputs(KTEPS) 356.39\n",
      "Epoch 00104 | Time(s) 0.0297 | Loss 0.3352 | F1 0.8376 | ETputs(KTEPS) 355.86\n",
      "Epoch 00105 | Time(s) 0.0297 | Loss 0.3415 | F1 0.8450 | ETputs(KTEPS) 355.69\n",
      "Epoch 00106 | Time(s) 0.0298 | Loss 0.3517 | F1 0.8450 | ETputs(KTEPS) 354.65\n",
      "Epoch 00107 | Time(s) 0.0299 | Loss 0.3467 | F1 0.8450 | ETputs(KTEPS) 353.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00108 | Time(s) 0.0300 | Loss 0.3468 | F1 0.8450 | ETputs(KTEPS) 352.27\n",
      "Epoch 00109 | Time(s) 0.0299 | Loss 0.3532 | F1 0.8376 | ETputs(KTEPS) 352.60\n",
      "Epoch 00110 | Time(s) 0.0300 | Loss 0.3306 | F1 0.8376 | ETputs(KTEPS) 352.33\n",
      "Epoch 00111 | Time(s) 0.0300 | Loss 0.3184 | F1 0.8376 | ETputs(KTEPS) 351.70\n",
      "Epoch 00112 | Time(s) 0.0300 | Loss 0.3076 | F1 0.8376 | ETputs(KTEPS) 351.30\n",
      "Epoch 00113 | Time(s) 0.0301 | Loss 0.3142 | F1 0.8376 | ETputs(KTEPS) 350.13\n",
      "Epoch 00114 | Time(s) 0.0301 | Loss 0.3289 | F1 0.8376 | ETputs(KTEPS) 350.16\n",
      "Epoch 00115 | Time(s) 0.0301 | Loss 0.3236 | F1 0.8413 | ETputs(KTEPS) 350.19\n",
      "Epoch 00116 | Time(s) 0.0303 | Loss 0.3301 | F1 0.8376 | ETputs(KTEPS) 348.83\n",
      "Epoch 00117 | Time(s) 0.0303 | Loss 0.3250 | F1 0.8339 | ETputs(KTEPS) 348.45\n",
      "Epoch 00118 | Time(s) 0.0303 | Loss 0.3270 | F1 0.8376 | ETputs(KTEPS) 348.68\n",
      "Epoch 00119 | Time(s) 0.0303 | Loss 0.3038 | F1 0.8413 | ETputs(KTEPS) 348.72\n",
      "Epoch 00120 | Time(s) 0.0303 | Loss 0.3096 | F1 0.8413 | ETputs(KTEPS) 348.75\n",
      "Epoch 00121 | Time(s) 0.0303 | Loss 0.3292 | F1 0.8413 | ETputs(KTEPS) 348.60\n",
      "Epoch 00122 | Time(s) 0.0303 | Loss 0.3174 | F1 0.8413 | ETputs(KTEPS) 348.51\n",
      "Epoch 00123 | Time(s) 0.0303 | Loss 0.3073 | F1 0.8413 | ETputs(KTEPS) 348.23\n",
      "Epoch 00124 | Time(s) 0.0303 | Loss 0.3151 | F1 0.8376 | ETputs(KTEPS) 348.15\n",
      "Epoch 00125 | Time(s) 0.0303 | Loss 0.3056 | F1 0.8376 | ETputs(KTEPS) 348.26\n",
      "Epoch 00126 | Time(s) 0.0303 | Loss 0.3090 | F1 0.8376 | ETputs(KTEPS) 348.01\n",
      "Epoch 00127 | Time(s) 0.0303 | Loss 0.2979 | F1 0.8376 | ETputs(KTEPS) 348.16\n",
      "Epoch 00128 | Time(s) 0.0303 | Loss 0.2995 | F1 0.8413 | ETputs(KTEPS) 347.98\n",
      "Epoch 00129 | Time(s) 0.0303 | Loss 0.3006 | F1 0.8376 | ETputs(KTEPS) 348.08\n",
      "Epoch 00130 | Time(s) 0.0303 | Loss 0.3063 | F1 0.8376 | ETputs(KTEPS) 348.03\n",
      "Epoch 00131 | Time(s) 0.0303 | Loss 0.3098 | F1 0.8376 | ETputs(KTEPS) 347.74\n",
      "Epoch 00132 | Time(s) 0.0303 | Loss 0.3243 | F1 0.8413 | ETputs(KTEPS) 347.87\n",
      "Epoch 00133 | Time(s) 0.0303 | Loss 0.2947 | F1 0.8413 | ETputs(KTEPS) 347.85\n",
      "Epoch 00134 | Time(s) 0.0303 | Loss 0.2991 | F1 0.8413 | ETputs(KTEPS) 348.06\n",
      "Epoch 00135 | Time(s) 0.0303 | Loss 0.3251 | F1 0.8413 | ETputs(KTEPS) 348.06\n",
      "Epoch 00136 | Time(s) 0.0304 | Loss 0.2929 | F1 0.8413 | ETputs(KTEPS) 347.61\n",
      "Epoch 00137 | Time(s) 0.0304 | Loss 0.3018 | F1 0.8413 | ETputs(KTEPS) 347.72\n",
      "Epoch 00138 | Time(s) 0.0304 | Loss 0.3047 | F1 0.8413 | ETputs(KTEPS) 347.72\n",
      "Epoch 00139 | Time(s) 0.0303 | Loss 0.3061 | F1 0.8376 | ETputs(KTEPS) 348.08\n",
      "Epoch 00140 | Time(s) 0.0303 | Loss 0.2983 | F1 0.8376 | ETputs(KTEPS) 348.25\n",
      "Epoch 00141 | Time(s) 0.0303 | Loss 0.3086 | F1 0.8413 | ETputs(KTEPS) 347.76\n",
      "Epoch 00142 | Time(s) 0.0303 | Loss 0.3093 | F1 0.8413 | ETputs(KTEPS) 348.09\n",
      "Epoch 00143 | Time(s) 0.0303 | Loss 0.3055 | F1 0.8376 | ETputs(KTEPS) 348.22\n",
      "Epoch 00144 | Time(s) 0.0303 | Loss 0.2549 | F1 0.8376 | ETputs(KTEPS) 347.87\n",
      "Epoch 00145 | Time(s) 0.0303 | Loss 0.2971 | F1 0.8413 | ETputs(KTEPS) 348.03\n",
      "Epoch 00146 | Time(s) 0.0303 | Loss 0.3045 | F1 0.8450 | ETputs(KTEPS) 347.81\n",
      "Epoch 00147 | Time(s) 0.0303 | Loss 0.2907 | F1 0.8413 | ETputs(KTEPS) 347.99\n",
      "Epoch 00148 | Time(s) 0.0303 | Loss 0.2764 | F1 0.8376 | ETputs(KTEPS) 348.26\n",
      "Epoch 00149 | Time(s) 0.0303 | Loss 0.2887 | F1 0.8376 | ETputs(KTEPS) 348.55\n",
      "Epoch 00150 | Time(s) 0.0302 | Loss 0.2853 | F1 0.8339 | ETputs(KTEPS) 348.90\n",
      "Epoch 00151 | Time(s) 0.0302 | Loss 0.2855 | F1 0.8339 | ETputs(KTEPS) 349.14\n",
      "Epoch 00152 | Time(s) 0.0302 | Loss 0.3043 | F1 0.8339 | ETputs(KTEPS) 349.45\n",
      "Epoch 00153 | Time(s) 0.0302 | Loss 0.2764 | F1 0.8376 | ETputs(KTEPS) 349.82\n",
      "Epoch 00154 | Time(s) 0.0302 | Loss 0.2777 | F1 0.8376 | ETputs(KTEPS) 349.66\n",
      "Epoch 00155 | Time(s) 0.0302 | Loss 0.2839 | F1 0.8339 | ETputs(KTEPS) 350.00\n",
      "Epoch 00156 | Time(s) 0.0301 | Loss 0.2627 | F1 0.8339 | ETputs(KTEPS) 350.30\n",
      "Epoch 00157 | Time(s) 0.0302 | Loss 0.2851 | F1 0.8339 | ETputs(KTEPS) 349.17\n",
      "Epoch 00158 | Time(s) 0.0305 | Loss 0.2750 | F1 0.8339 | ETputs(KTEPS) 346.51\n",
      "Epoch 00159 | Time(s) 0.0306 | Loss 0.2992 | F1 0.8413 | ETputs(KTEPS) 345.32\n",
      "Epoch 00160 | Time(s) 0.0307 | Loss 0.2824 | F1 0.8413 | ETputs(KTEPS) 343.59\n",
      "Epoch 00161 | Time(s) 0.0308 | Loss 0.2734 | F1 0.8413 | ETputs(KTEPS) 342.65\n",
      "Epoch 00162 | Time(s) 0.0308 | Loss 0.2610 | F1 0.8376 | ETputs(KTEPS) 342.56\n",
      "Epoch 00163 | Time(s) 0.0309 | Loss 0.2685 | F1 0.8339 | ETputs(KTEPS) 341.11\n",
      "Epoch 00164 | Time(s) 0.0310 | Loss 0.2733 | F1 0.8339 | ETputs(KTEPS) 340.70\n",
      "Epoch 00165 | Time(s) 0.0310 | Loss 0.2719 | F1 0.8339 | ETputs(KTEPS) 340.51\n",
      "Epoch 00166 | Time(s) 0.0310 | Loss 0.2601 | F1 0.8303 | ETputs(KTEPS) 340.41\n",
      "Epoch 00167 | Time(s) 0.0310 | Loss 0.2868 | F1 0.8339 | ETputs(KTEPS) 340.38\n",
      "Epoch 00168 | Time(s) 0.0310 | Loss 0.2628 | F1 0.8339 | ETputs(KTEPS) 340.31\n",
      "Epoch 00169 | Time(s) 0.0310 | Loss 0.2707 | F1 0.8376 | ETputs(KTEPS) 340.14\n",
      "Epoch 00170 | Time(s) 0.0310 | Loss 0.2677 | F1 0.8339 | ETputs(KTEPS) 340.00\n",
      "Epoch 00171 | Time(s) 0.0313 | Loss 0.2844 | F1 0.8339 | ETputs(KTEPS) 337.38\n",
      "Epoch 00172 | Time(s) 0.0313 | Loss 0.2857 | F1 0.8339 | ETputs(KTEPS) 337.08\n",
      "Epoch 00173 | Time(s) 0.0314 | Loss 0.2736 | F1 0.8339 | ETputs(KTEPS) 336.24\n",
      "Epoch 00174 | Time(s) 0.0314 | Loss 0.2653 | F1 0.8413 | ETputs(KTEPS) 336.11\n",
      "Epoch 00175 | Time(s) 0.0314 | Loss 0.2726 | F1 0.8413 | ETputs(KTEPS) 336.58\n",
      "Epoch 00176 | Time(s) 0.0313 | Loss 0.2700 | F1 0.8487 | ETputs(KTEPS) 337.19\n",
      "Epoch 00177 | Time(s) 0.0313 | Loss 0.3018 | F1 0.8450 | ETputs(KTEPS) 337.71\n",
      "Epoch 00178 | Time(s) 0.0312 | Loss 0.2684 | F1 0.8413 | ETputs(KTEPS) 338.30\n",
      "Epoch 00179 | Time(s) 0.0311 | Loss 0.2643 | F1 0.8376 | ETputs(KTEPS) 338.90\n",
      "Epoch 00180 | Time(s) 0.0311 | Loss 0.2656 | F1 0.8339 | ETputs(KTEPS) 339.30\n",
      "Epoch 00181 | Time(s) 0.0311 | Loss 0.2712 | F1 0.8339 | ETputs(KTEPS) 339.78\n",
      "Epoch 00182 | Time(s) 0.0310 | Loss 0.2678 | F1 0.8339 | ETputs(KTEPS) 340.18\n",
      "Epoch 00183 | Time(s) 0.0310 | Loss 0.2574 | F1 0.8339 | ETputs(KTEPS) 340.69\n",
      "Epoch 00184 | Time(s) 0.0310 | Loss 0.2735 | F1 0.8376 | ETputs(KTEPS) 340.78\n",
      "Epoch 00185 | Time(s) 0.0309 | Loss 0.2734 | F1 0.8376 | ETputs(KTEPS) 341.19\n",
      "Epoch 00186 | Time(s) 0.0309 | Loss 0.2805 | F1 0.8413 | ETputs(KTEPS) 341.67\n",
      "Epoch 00187 | Time(s) 0.0309 | Loss 0.2541 | F1 0.8413 | ETputs(KTEPS) 342.05\n",
      "Epoch 00188 | Time(s) 0.0308 | Loss 0.2645 | F1 0.8450 | ETputs(KTEPS) 342.54\n",
      "Epoch 00189 | Time(s) 0.0308 | Loss 0.2764 | F1 0.8376 | ETputs(KTEPS) 342.78\n",
      "Epoch 00190 | Time(s) 0.0308 | Loss 0.2701 | F1 0.8339 | ETputs(KTEPS) 343.13\n",
      "Epoch 00191 | Time(s) 0.0307 | Loss 0.2481 | F1 0.8376 | ETputs(KTEPS) 343.36\n",
      "Epoch 00192 | Time(s) 0.0307 | Loss 0.2692 | F1 0.8413 | ETputs(KTEPS) 343.48\n",
      "Epoch 00193 | Time(s) 0.0307 | Loss 0.2543 | F1 0.8413 | ETputs(KTEPS) 343.67\n",
      "Epoch 00194 | Time(s) 0.0307 | Loss 0.2683 | F1 0.8413 | ETputs(KTEPS) 344.13\n",
      "Epoch 00195 | Time(s) 0.0306 | Loss 0.2728 | F1 0.8487 | ETputs(KTEPS) 344.59\n",
      "Epoch 00196 | Time(s) 0.0306 | Loss 0.2424 | F1 0.8487 | ETputs(KTEPS) 345.11\n",
      "Epoch 00197 | Time(s) 0.0306 | Loss 0.2671 | F1 0.8487 | ETputs(KTEPS) 344.74\n",
      "Epoch 00198 | Time(s) 0.0306 | Loss 0.2676 | F1 0.8450 | ETputs(KTEPS) 345.15\n",
      "Epoch 00199 | Time(s) 0.0305 | Loss 0.2639 | F1 0.8413 | ETputs(KTEPS) 345.50\n",
      "\n",
      "Test F1 0.87\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8692762186115215"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Cora()\n",
    "transformer = TFIDF()\n",
    "dataset.transform_features(transformer)\n",
    "train_gcn(dataset, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:graph_text]",
   "language": "python",
   "name": "conda-env-graph_text-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
