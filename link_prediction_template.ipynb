{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"GCN using DGL nn package\n",
    "\n",
    "References:\n",
    "- Semi-Supervised Classification with Graph Convolutional Networks\n",
    "- Paper: https://arxiv.org/abs/1609.02907\n",
    "- Code: https://github.com/tkipf/gcn\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 g,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 activation,\n",
    "                 dropout=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.g = g\n",
    "\n",
    "        self.gcn_layer1 = GraphConv(in_feats, n_hidden, activation=activation)\n",
    "\n",
    "        self.gcn_layer2 = GraphConv(n_hidden, n_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, features):\n",
    "        h = features\n",
    "\n",
    "        h = self.gcn_layer1(self.g, h)\n",
    "\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        h = self.gcn_layer2(self.g, h)\n",
    "\n",
    "        return h\n",
    "\n",
    "    def freeze_features(self, freeze):\n",
    "        self.emb.weight.requires_grad = not freeze\n",
    "\n",
    "    def freeze_graph(self, freeze):\n",
    "        self.gcn_layer1.weight.requires_grad = not freeze\n",
    "        self.gcn_layer2.weight.requires_grad = not freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(n,\n",
    "              main_ids,\n",
    "              main_labels,\n",
    "              test_ratio,\n",
    "              val_ratio,\n",
    "              seed=1):\n",
    "    \"\"\"\n",
    "    Randomly splits data into train/val/test using random seed\n",
    "    returns masks instead of the data itself  \n",
    "    \"\"\"\n",
    "    train_mask = np.zeros(n)\n",
    "    val_mask = np.zeros(n)\n",
    "    test_mask = np.zeros(n)\n",
    "\n",
    "    x_dev, x_test, y_dev, y_test = train_test_split(main_ids,\n",
    "                                                    main_labels,\n",
    "                                                    stratify=main_labels,\n",
    "                                                    test_size=test_ratio,\n",
    "                                                    random_state=seed)\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_dev,\n",
    "                                                      y_dev,\n",
    "                                                      stratify=y_dev,\n",
    "                                                      test_size=val_ratio,\n",
    "                                                      random_state=seed)\n",
    "\n",
    "    train_mask[x_train] = 1\n",
    "    val_mask[x_val] = 1\n",
    "    test_mask[x_test] = 1\n",
    "\n",
    "    return train_mask, val_mask, test_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, features, labels, mask):\n",
    "    \"\"\"\n",
    "    Evaluate model quality (F1-score)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask].detach().cpu().numpy()\n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        predicted = predicted.detach().cpu().numpy()\n",
    "        f1 = f1_score(labels, predicted, average='micro')\n",
    "        return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REPLACE HERE WITH YOUR MODEL\n",
    "\n",
    "MODEL = GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dgl\n",
    "from dgl import DGLGraph\n",
    "\n",
    "def train_gcn_lp(graph,\n",
    "              features,\n",
    "              labels,\n",
    "              seed=1,\n",
    "              n_hidden=64,\n",
    "              n_epochs=200,\n",
    "              lr=1e-2,\n",
    "              weight_decay=5e-4,\n",
    "              dropout=0.5,\n",
    "              verbose=True,\n",
    "              cuda=False):\n",
    "\n",
    "    features = torch.FloatTensor(features)\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    n = len(data['ids'])\n",
    "    \n",
    "    mask = []\n",
    "    for i in range(len(labels)):\n",
    "        # nodes with labels\n",
    "        if graph.nodes[i]['is_main']:\n",
    "            mask.append(1)\n",
    "        else:\n",
    "            mask.append(0)\n",
    "            \n",
    "    mask = torch.BoolTensor(mask)\n",
    "        \n",
    "\n",
    "    if cuda:\n",
    "        torch.cuda.set_device(\"cuda:0\")\n",
    "        features = features.cuda()\n",
    "        labels = labels.cuda()\n",
    "        train_mask = train_mask.cuda()\n",
    "        val_mask = val_mask.cuda()\n",
    "        test_mask = test_mask.cuda()\n",
    "\n",
    "    g = DGLGraph(graph)\n",
    "    g = dgl.transform.add_self_loop(g)\n",
    "    n_edges = g.number_of_edges()\n",
    "\n",
    "    degs = g.in_degrees().float()\n",
    "    norm = torch.pow(degs, -0.5)\n",
    "    norm[torch.isinf(norm)] = 0\n",
    "\n",
    "    if cuda:\n",
    "        norm = norm.cuda()\n",
    "\n",
    "    g.ndata['norm'] = norm.unsqueeze(1)\n",
    "\n",
    "    in_feats = features.shape[1]\n",
    "\n",
    "    # + 1 for unknown class\n",
    "    n_classes = data['n_classes'] + 1\n",
    "    \n",
    "    ##########\n",
    "    ##########  HERE WE USE MODEL\n",
    "    ##########\n",
    "    model = MODEL(g,\n",
    "                in_feats=in_feats,\n",
    "                n_hidden=n_hidden,\n",
    "                n_classes=n_classes,\n",
    "                activation=F.relu,\n",
    "                dropout=dropout)\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # use optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=lr,\n",
    "                                 weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                           mode='min',\n",
    "                                                           factor=0.9,\n",
    "                                                           patience=20,\n",
    "                                                           min_lr=1e-10)\n",
    "\n",
    "    best_f1 = -100\n",
    "    # initialize graph\n",
    "    dur = []\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        if epoch >= 3:\n",
    "            t0 = time.time()\n",
    "        logits = model(features)\n",
    "        loss = loss_fcn(logits[mask], labels[mask])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch >= 3:\n",
    "            dur.append(time.time() - t0)\n",
    "\n",
    "        f1 = evaluate(model, features, labels, mask)\n",
    "        scheduler.step(1 - f1)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | F1 {:.4f} | \"\n",
    "                  \"ETputs(KTEPS) {:.2f}\".format(epoch, np.mean(dur), loss.item(),\n",
    "                                                f1, n_edges / np.mean(dur) / 1000))\n",
    "\n",
    "    model.load_state_dict(torch.load('best_model.pt'))\n",
    "    \n",
    "    embeddings = model.gcn_layer1(model.g, model.apply_embs(features)).detach().cpu().numpy()\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.base_model import BaseModel\n",
    "\n",
    "class GCN_Model_LP(BaseModel):\n",
    "    def __init__(self, graph, features, labels=None, dim=80):\n",
    "        super(GCN_Model_LP, self).__init__(graph, features, dim, labels)\n",
    "\n",
    "    def learn_embeddings(self):\n",
    "        embeddings = train_gcn_lp(self.graph, self.features, self.labels)\n",
    "        self.embeddings = embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Cora, CiteseerM10, Dblp\n",
    "\n",
    "datasets = [\n",
    "   ('Cora', Cora),\n",
    "   # ('CiteseerM10', CiteseerM10),\n",
    "   # ('DBLP', Dblp)\n",
    "]\n",
    "\n",
    "\n",
    "from text_transformers import SBert, LDA, W2V, Sent2Vec, Doc2Vec, BOW, TFIDF\n",
    "\n",
    "tasks = [\n",
    "    ('GCN (W2V)', lambda ds: LpTask(ds, test_ratios, W2V, GCN_Model_LP, d=100, labels=True))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1]\n",
    "test_ratios = [0.5, 0.7, 0.9, 0.95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "datasets:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Tasks:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "test_ratios:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "seeds:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-e4c9e2f48dc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtask_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_constr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Tasks'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_constr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtask_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtest_ratio\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtask_res\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_ratio\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hse/year_2/thesis/graph_text/task/lp_task.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m                                                       \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlp_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                                                       dim=self.d)\n\u001b[0;32m---> 50\u001b[0;31m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-2a84fc0963ac>\u001b[0m in \u001b[0;36mlearn_embeddings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlearn_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gcn_lp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-b2e76032b2c6>\u001b[0m in \u001b[0;36mtrain_gcn_lp\u001b[0;34m(graph, features, labels, seed, n_hidden, n_epochs, lr, weight_decay, dropout, verbose, cuda)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from task import LpTask\n",
    "\n",
    "res = {}\n",
    "\n",
    "for ds_name, ds_constr in tqdm(datasets, desc='datasets'):\n",
    "    ds = ds_constr()\n",
    "    for task_name, task_constr in tqdm(tasks, desc='Tasks'):\n",
    "        task = task_constr(ds)\n",
    "        task_res = task.evaluate()\n",
    "        for test_ratio in task_res:\n",
    "            scores = task_res[test_ratio]\n",
    "            res[f'{1 - test_ratio} - {ds_name} - {task_name}'] = scores\n",
    "\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, scores in res.items():\n",
    "    print(name, scores, np.mean(scores), np.std(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:graph_text]",
   "language": "python",
   "name": "conda-env-graph_text-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
