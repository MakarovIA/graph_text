{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:graph_text]",
      "language": "python",
      "name": "conda-env-graph_text-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Copy of experiments.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY2rK7_tmyjV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e168110-fc9a-49aa-bd3a-4bd62313afa7"
      },
      "source": [
        "!rm -rf graph_text\n",
        "!git clone https://github.com/mikhmakarov/graph_text.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'graph_text'...\n",
            "remote: Enumerating objects: 408, done.\u001b[K\n",
            "remote: Counting objects: 100% (408/408), done.\u001b[K\n",
            "remote: Compressing objects: 100% (273/273), done.\u001b[K\n",
            "remote: Total 408 (delta 236), reused 292 (delta 127), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (408/408), 6.23 MiB | 25.60 MiB/s, done.\n",
            "Resolving deltas: 100% (236/236), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDb6F78NnSYZ",
        "outputId": "a216afd5-2e65-42f9-84cc-e06765ef6424"
      },
      "source": [
        "!pip install sentence_transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.6/dist-packages (0.4.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence_transformers) (1.19.5)\n",
            "Requirement already satisfied: transformers<5.0.0,>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from sentence_transformers) (4.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence_transformers) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence_transformers) (4.41.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence_transformers) (3.2.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from sentence_transformers) (0.1.95)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from sentence_transformers) (1.7.0+cu101)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence_transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (20.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (0.9.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence_transformers) (0.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence_transformers) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence_transformers) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence_transformers) (3.7.4.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence_transformers) (1.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence_transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence_transformers) (2020.12.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence_transformers) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7T6UlrynOls",
        "outputId": "3c49938e-9ab3-43ad-aa30-521db6d1477a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dugopY-gmx2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40f57a87-c5a4-4bca-f110-f2890d4d5340"
      },
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(\"graph_text\")\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# from models import TADW, TriDnr, DeepWalk, Node2Vec, Hope\n",
        "from text_transformers import Ernie\n",
        "from datasets import Cora, CiteseerM10, Dblp\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "from task import Task"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Going to train on cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAMb3Obpmx2v"
      },
      "source": [
        "datasets = [\n",
        "    ('Cora', Cora),\n",
        "   # ('CiteseerM10', CiteseerM10),\n",
        "  #   ('DBLP', Dblp)\n",
        "]\n",
        "\n",
        "test_ratios = [0.5, 0.7, 0.9, 0.95]\n",
        "\n",
        "\n",
        "tasks = [\n",
        "    ('Ernie', lambda ds: Task(ds, test_ratios, lambda: Ernie(), None, d=None, labels=False)),\n",
        "    # ('BOW', lambda ds: Task(ds, test_ratios, lambda: BOW(), None, d=None, labels=False)),\n",
        "    # ('TFIDF', lambda ds: Task(ds, test_ratios, lambda: TFIDF(), None, d=None, labels=False)),\n",
        "    # ('LDA', lambda ds: Task(ds, test_ratios, lambda: LDA(), None, d=None, labels=False)),\n",
        "    # ('SBERT pretrained', lambda ds: Task(ds, test_ratios, lambda: SBert(train=False, d=300), None, d=None, labels=False)),\n",
        "    # ('W2V pretrained (d=300)', lambda ds: Task(ds, test_ratios, lambda: W2V(train=False, d=300), None, d=None, labels=False)),\n",
        "    # ('W2V (d=300)', lambda ds: Task(ds, test_ratios, lambda: W2V(train=True, d=300), None, d=None, labels=False)),\n",
        "    # ('W2V (d=64)', lambda ds: Task(ds, test_ratios, lambda: W2V(train=True, d=64), None, d=None, labels=False)),\n",
        "    # ('Doc2Vec pretrained (d=300)', lambda ds: Task(ds, test_ratios, lambda: Doc2Vec(train=False, d=300), None, d=None, labels=False)),\n",
        "    # ('Doc2Vec (d=300)', lambda ds: Task(ds, test_ratios, lambda: Doc2Vec(train=True, d=300), None, d=None, labels=False)),\n",
        "    # ('Doc2Vec (d=64)', lambda ds: Task(ds, test_ratios, lambda: Doc2Vec(train=True, d=64), None, d=None, labels=False)),\n",
        "    # ('Sent2Vec pretrained (d=600)', lambda ds: Task(ds, test_ratios, lambda: Sent2Vec(train=False, d=600), None, d=None, labels=False)),\n",
        "    # ('Sent2Vec (d=600)', lambda ds: Task(ds, test_ratios, lambda: Sent2Vec(train=True, d=600), None, d=None, labels=False)),\n",
        "    # ('Sent2Vec (d=64)', lambda ds: Task(ds, test_ratios, lambda: Sent2Vec(train=True, d=64), None, d=None, labels=False)),\n",
        "    # ('DeepWalk (d=100)', lambda ds: Task(ds, test_ratios, None, DeepWalk, d=100, labels=False)),\n",
        "    # ('Node2Vec (d=100)', lambda ds: Task(ds, test_ratios, None, Node2Vec, d=100, labels=False)),\n",
        "    # ('Hope (d=100)', lambda ds: Task(ds, test_ratios, None, Hope, d=100, labels=False)),\n",
        "    # ('TADW - BOW', lambda ds: Task(ds, test_ratios, BOW, TADW, d=160, labels=False)),\n",
        "    # ('TADW - TFIDF', lambda ds: Task(ds, test_ratios, TFIDF, TADW, d=160, labels=False)),\n",
        "    # ('TADW - Sent2Vec', lambda ds: Task(ds, test_ratios, lambda: Sent2Vec(train=True, d=64), TADW, d=160, labels=False)),\n",
        "    # ('TADW - Word2Vec', lambda ds: Task(ds, test_ratios, lambda: W2V(train=True, d=64), TADW, d=160, labels=False)),\n",
        "    # ('TriDNR', lambda ds: Task(ds, test_ratios, None, TriDnr, d=160, labels=True)),\n",
        "    # ('BOW:DeepWalk', lambda ds: Task(ds, test_ratios, BOW, DeepWalk, d=100,\n",
        "    #                                       labels=False, concat=True)),\n",
        "    # ('Word2Vec:DeepWalk', lambda ds: Task(ds, test_ratios, lambda: W2V(train=True, d=64), DeepWalk, d=100,\n",
        "    #                                       labels=False, concat=True)),\n",
        "    # ('Sent2Vec:DeepWalk', lambda ds: Task(ds, test_ratios, lambda: Sent2Vec(train=True, d=64), DeepWalk, d=100,\n",
        "    #                                       labels=False, concat=True)),\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y5cd1xBmx2w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4be1d75-b713-43f5-f51c-3ecc2d8c09cd"
      },
      "source": [
        "res = {}\n",
        "\n",
        "for ds_name, ds_constr in tqdm(datasets, desc='datasets'):\n",
        "    ds = ds_constr()\n",
        "    for task_name, task_constr in tqdm(tasks, desc='Tasks'):\n",
        "        task = task_constr(ds)\n",
        "        task_res = task.evaluate()\n",
        "        for test_ratio in task_res:\n",
        "            scores = task_res[test_ratio]\n",
        "            res[f'{1 - test_ratio} - {ds_name} - {task_name}'] = scores\n",
        "\n",
        "for name, scores in res.items():\n",
        "    print(name, round(np.mean(scores), 3), round(np.std(scores), 3))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datasets:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Tasks:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "1it [00:00,  2.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "2it [00:01,  1.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "3it [00:01,  1.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "4it [00:01,  2.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "5it [00:02,  2.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "6it [00:02,  2.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "7it [00:03,  2.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "8it [00:03,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "9it [00:04,  1.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "10it [00:04,  2.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "11it [00:05,  1.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "12it [00:05,  1.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "13it [00:06,  1.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "14it [00:06,  2.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "15it [00:07,  1.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "16it [00:08,  1.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "17it [00:09,  1.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "18it [00:09,  1.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "19it [00:10,  1.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "20it [00:11,  1.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "21it [00:11,  1.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "22it [00:12,  1.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "23it [00:12,  1.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "24it [00:13,  1.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "25it [00:13,  1.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "26it [00:14,  1.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "27it [00:14,  1.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "28it [00:15,  1.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "29it [00:15,  1.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "30it [00:16,  1.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "31it [00:16,  2.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "32it [00:16,  2.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "33it [00:17,  2.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "34it [00:17,  2.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "35it [00:18,  1.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "36it [00:18,  1.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "37it [00:19,  1.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "38it [00:19,  2.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "39it [00:20,  2.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "40it [00:20,  2.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "41it [00:21,  2.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "42it [00:21,  2.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "43it [00:22,  2.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "44it [00:22,  1.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "45it [00:23,  1.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "46it [00:24,  1.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "47it [00:24,  1.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "48it [00:25,  1.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "49it [00:25,  1.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "50it [00:26,  2.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "51it [00:26,  2.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "52it [00:27,  2.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "53it [00:27,  2.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "54it [00:27,  2.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "55it [00:28,  2.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "56it [00:29,  1.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "57it [00:29,  1.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "58it [00:30,  2.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "59it [00:30,  2.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "60it [00:30,  2.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "61it [00:31,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "62it [00:31,  2.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "63it [00:32,  2.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "64it [00:32,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "65it [00:33,  2.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "66it [00:33,  2.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "67it [00:33,  2.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "68it [00:34,  2.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "69it [00:34,  2.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "70it [00:35,  1.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "71it [00:36,  1.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "72it [00:36,  1.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "73it [00:36,  1.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "74it [00:37,  2.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "75it [00:37,  1.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "76it [00:38,  1.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "77it [00:39,  1.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "78it [00:39,  1.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "79it [00:40,  1.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "80it [00:40,  1.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "81it [00:41,  1.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "82it [00:41,  2.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "83it [00:42,  1.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "84it [00:43,  1.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "85it [00:43,  1.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "86it [00:43,  2.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "87it [00:44,  2.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "88it [00:44,  2.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "89it [00:45,  2.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "90it [00:45,  2.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "91it [00:46,  2.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "92it [00:46,  2.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "93it [00:47,  1.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "94it [00:47,  2.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "95it [00:48,  2.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "96it [00:48,  2.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "97it [00:49,  2.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "98it [00:49,  2.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "99it [00:49,  2.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "100it [00:50,  2.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "101it [00:50,  2.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "102it [00:51,  2.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "103it [00:51,  2.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "104it [00:52,  1.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "105it [00:52,  1.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "106it [00:53,  1.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "107it [00:53,  1.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "108it [00:54,  2.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "109it [00:54,  1.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "110it [00:55,  1.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "111it [00:56,  1.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "112it [00:56,  1.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "113it [00:57,  1.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "114it [00:58,  1.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "115it [00:58,  1.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "116it [00:59,  1.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "117it [00:59,  1.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "118it [01:00,  1.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "119it [01:01,  1.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "120it [01:01,  1.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "121it [01:02,  1.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "122it [01:02,  1.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "123it [01:03,  1.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "124it [01:04,  1.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "125it [01:04,  1.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "126it [01:05,  1.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "127it [01:05,  1.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "128it [01:06,  1.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "129it [01:07,  1.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "130it [01:07,  1.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "131it [01:07,  2.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "132it [01:08,  1.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "133it [01:09,  1.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "134it [01:09,  1.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "135it [01:10,  1.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "136it [01:10,  2.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "137it [01:10,  2.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "138it [01:11,  2.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "139it [01:11,  2.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "140it [01:12,  2.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "141it [01:13,  1.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "142it [01:14,  1.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "143it [01:15,  1.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "144it [01:15,  1.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "145it [01:16,  1.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "146it [01:16,  1.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "147it [01:17,  1.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "148it [01:18,  1.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "149it [01:19,  1.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "150it [01:19,  1.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "151it [01:20,  1.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "152it [01:21,  1.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "153it [01:21,  1.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "154it [01:22,  1.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "155it [01:23,  1.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "156it [01:23,  1.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "157it [01:23,  1.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "158it [01:24,  1.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "159it [01:24,  1.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "160it [01:25,  1.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "161it [01:25,  2.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "162it [01:26,  1.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "163it [01:26,  1.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "164it [01:27,  2.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "165it [01:27,  2.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "166it [01:28,  2.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "167it [01:28,  1.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "168it [01:29,  2.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "170it [01:29,  1.89it/s]\n",
            "\n",
            "\n",
            "test_ratios:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "seeds:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "seeds:  20%|██        | 1/5 [00:00<00:03,  1.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "seeds:  40%|████      | 2/5 [00:01<00:02,  1.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "seeds:  60%|██████    | 3/5 [00:02<00:01,  1.11it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "seeds:  80%|████████  | 4/5 [00:03<00:00,  1.09it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "seeds: 100%|██████████| 5/5 [00:04<00:00,  1.09it/s]\n",
            "\n",
            "\n",
            "test_ratios:  25%|██▌       | 1/4 [00:04<00:13,  4.61s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "seeds:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "seeds:  20%|██        | 1/5 [00:00<00:02,  1.36it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "seeds:  40%|████      | 2/5 [00:01<00:02,  1.34it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "seeds:  60%|██████    | 3/5 [00:02<00:01,  1.35it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "seeds:  80%|████████  | 4/5 [00:02<00:00,  1.35it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "seeds: 100%|██████████| 5/5 [00:03<00:00,  1.34it/s]\n",
            "\n",
            "\n",
            "test_ratios:  50%|█████     | 2/4 [00:08<00:08,  4.35s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "seeds:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "seeds:  20%|██        | 1/5 [00:00<00:02,  1.74it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "seeds:  40%|████      | 2/5 [00:01<00:01,  1.77it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "seeds:  60%|██████    | 3/5 [00:01<00:01,  1.78it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "seeds:  80%|████████  | 4/5 [00:02<00:00,  1.83it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "seeds: 100%|██████████| 5/5 [00:02<00:00,  1.83it/s]\n",
            "\n",
            "\n",
            "test_ratios:  75%|███████▌  | 3/4 [00:11<00:03,  3.86s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "seeds:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "seeds:  20%|██        | 1/5 [00:00<00:01,  2.08it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "seeds:  40%|████      | 2/5 [00:00<00:01,  2.10it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "seeds:  60%|██████    | 3/5 [00:01<00:00,  2.11it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "seeds:  80%|████████  | 4/5 [00:01<00:00,  2.12it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "seeds: 100%|██████████| 5/5 [00:02<00:00,  2.12it/s]\n",
            "\n",
            "\n",
            "test_ratios: 100%|██████████| 4/4 [00:13<00:00,  3.36s/it]\n",
            "\n",
            "Tasks: 100%|██████████| 1/1 [01:51<00:00, 111.89s/it]\n",
            "datasets: 100%|██████████| 1/1 [01:51<00:00, 111.97s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.5 - Cora - Ernie [0.6432791728212703, 0.6654357459379616, 0.6521418020679468, 0.6617429837518464, 0.6491875923190547] 0.654357459379616 0.008144194240525978\n",
            "0.30000000000000004 - Cora - Ernie [0.6054852320675106, 0.6223628691983122, 0.6118143459915611, 0.620253164556962, 0.620253164556962] 0.6160337552742614 0.006399051007638412\n",
            "0.09999999999999998 - Cora - Ernie [0.5057424118129614, 0.5004101722723544, 0.5127153404429861, 0.5377358490566038, 0.5451189499589828] 0.5203445447087777 0.017804689661380733\n",
            "0.050000000000000044 - Cora - Ernie [0.43179168286047415, 0.4232413525068014, 0.4224640497473766, 0.43878740769529734, 0.4741546832491255] 0.438087835211815 0.019002469231057605\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43OnX1W6mx2x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42b55583-3d08-4e22-98be-f6081f3b1b9d"
      },
      "source": [
        "len([101, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0]), len([101, 102, 0, 0, 0, 0, 0, 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47q5fLn9xD82"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b-N6EtmxD_Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsCRlQS_xECR",
        "outputId": "c9f7b01f-fcaa-45ec-ce22-dcfb503be32e"
      },
      "source": [
        "!pip install tqdm boto3 requests regex sentencepiece sacremoses pytorch-transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (1.17.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.23.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.95)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (0.0.43)\n",
            "Collecting pytorch-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3) (0.3.4)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.1 in /usr/local/lib/python3.6/dist-packages (from boto3) (1.20.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.21.0,>=1.20.1->boto3) (2.8.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (0.8)\n",
            "Installing collected packages: pytorch-transformers\n",
            "Successfully installed pytorch-transformers-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irf5vXWtxEGO",
        "outputId": "4fc776ff-f4e2-46b6-9e42-7beddeef845d"
      },
      "source": [
        "import torch\n",
        "import logging\n",
        "from pytorch_transformers import BertModel, BertTokenizer\n",
        "from pytorch_transformers import *\n",
        "from typing import List\n",
        "\n",
        "CLS_TOKEN = \"[CLS]\"\n",
        "SEP_TOKEN = \"[SEP]\"\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "            label: (Optional) string. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples, tokenizer, max_seq_length,\n",
        "                                 cls_token_at_end=False, pad_on_left=False,\n",
        "                                 cls_token='[CLS]', sep_token='[SEP]', pad_token=0,\n",
        "                                 sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
        "                                 cls_token_segment_id=1, pad_token_segment_id=0,\n",
        "                                 mask_padding_with_zero=True):\n",
        "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
        "        `cls_token_at_end` define the location of the CLS token:\n",
        "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
        "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
        "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        if ex_index % 10000 == 0:\n",
        "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
        "\n",
        "        tokens_a = tokenizer.tokenize(example.text_a)\n",
        "\n",
        "        tokens_b = None\n",
        "        if example.text_b:\n",
        "            tokens_b = tokenizer.tokenize(example.text_b)\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "            # length is less than the specified length.\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "        else:\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\n",
        "            if len(tokens_a) > max_seq_length - 2:\n",
        "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "\n",
        "        # The convention in BERT is:\n",
        "        # (a) For sequence pairs:\n",
        "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
        "        # (b) For single sequences:\n",
        "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "        #  type_ids:   0   0   0   0  0     0   0\n",
        "        #\n",
        "        # Where \"type_ids\" are used to indicate whether this is the first\n",
        "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "        # embedding vector (and position vector). This is not *strictly* necessary\n",
        "        # since the [SEP] token unambiguously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "        #\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        tokens = tokens_a + [sep_token]\n",
        "        segment_ids = [sequence_a_segment_id] * len(tokens)\n",
        "\n",
        "        if tokens_b:\n",
        "            tokens += tokens_b + [sep_token]\n",
        "            segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
        "\n",
        "        if cls_token_at_end:\n",
        "            tokens = tokens + [cls_token]\n",
        "            segment_ids = segment_ids + [cls_token_segment_id]\n",
        "        else:\n",
        "            tokens = [cls_token] + tokens\n",
        "            segment_ids = [cls_token_segment_id] + segment_ids\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding_length = max_seq_length - len(input_ids)\n",
        "        if pad_on_left:\n",
        "            input_ids = ([pad_token] * padding_length) + input_ids\n",
        "            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
        "            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
        "        else:\n",
        "            input_ids = input_ids + ([pad_token] * padding_length)\n",
        "            input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
        "            segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "\n",
        "        if ex_index < 5:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % (example.guid))\n",
        "            logger.info(\"tokens: %s\" % \" \".join(\n",
        "                    [str(x) for x in tokens]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "            logger.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "\n",
        "        features.append(\n",
        "                InputFeatures(input_ids=input_ids,\n",
        "                              input_mask=input_mask,\n",
        "                              segment_ids=segment_ids))\n",
        "    return features\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()\n",
        "\n",
        "def select_field(features, field):\n",
        "    \"\"\"As the output is dic, return relevant field\"\"\"\n",
        "    return [[choice[field] for choice in feature.choices_features] for feature in features]\n",
        "\n",
        "\n",
        "def create_examples(_list, set_type=\"train\"):\n",
        "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "    examples = []\n",
        "    for (i, line) in enumerate(_list):\n",
        "        guid = \"%s-%s\" % (set_type, i)\n",
        "        text_a = line\n",
        "        # text_b = line[1]\n",
        "        examples.append(\n",
        "            InputExample(guid=guid, text_a=text_a))\n",
        "    return examples\n",
        "\n",
        "\n",
        "class BertEmbedder:\n",
        "    def __init__(self,\n",
        "                 pretrained_weights='bert-base-uncased',\n",
        "                 tokenizer_class=BertTokenizer,\n",
        "                 model_class=BertModel,\n",
        "                 max_seq_len=20):\n",
        "        super().__init__()\n",
        "        self.pretrained_weights = pretrained_weights\n",
        "        self.tokenizer_class = tokenizer_class\n",
        "        self.model_class = model_class\n",
        "        self.tokenizer = self.tokenizer_class.from_pretrained(pretrained_weights)\n",
        "        self.model = self.model_class.from_pretrained(pretrained_weights)\n",
        "        self.max_seq_len = max_seq_len\n",
        "        # tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
        "        # model = BertModel.from_pretrained(pretrained_weights)\n",
        "\n",
        "    def get_bert_embeddings(self,\n",
        "                            raw_text: List[str]) -> torch.tensor:\n",
        "        examples = create_examples(raw_text)\n",
        "\n",
        "        features = convert_examples_to_features(\n",
        "            examples, self.tokenizer, self.max_seq_len, True)\n",
        "\n",
        "        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "        all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "\n",
        "        last_hidden_states = self.model(all_input_ids)[0]  # Models outputs are now tuples\n",
        "        print(last_hidden_states.size())\n",
        "        return last_hidden_states\n",
        "\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    embedder = BertEmbedder()\n",
        "    raw_text = [\"[CLS] This is first element [SEP] continuing statement\",\n",
        "                \"[CLS] second element of the list.\"]\n",
        "    bert_embedding = embedder.get_bert_embeddings(raw_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 1138725.25B/s]\n",
            "100%|██████████| 433/433 [00:00<00:00, 217787.94B/s]\n",
            "100%|██████████| 440473133/440473133 [00:12<00:00, 34685819.44B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 20, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMXF-u6WxEtp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}